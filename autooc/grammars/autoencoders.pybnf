# GE keras autoencoder python grammar

<res> ::= encoder = Sequential(){::}encoder.add(Input(shape=(input_shape,))){::}<hidden_layers>{::}<latent_space>{::}model = get_model_from_encoder(encoder){::}model.add(Dense(input_shape)){::}model.compile("'Adam'", "'mse'")

<hidden_layers> ::= <Dense>{::} | <hidden_layers><Dense>{::} | <Dense>{::}encoder.add(Dropout(rate=0.<digit>)){::} | <Dense>{::}encoder.add(BatchNormalization()){::}

<Dense> ::= encoder.add(Dense(units = <DenseUnits>, activation = <activation>))

<DenseUnits> ::= 32 | 64 | 128 | 256 | 512

<activation> ::= "'relu'" | "'softplus'" | "'softsign'" | "'selu'" | "'elu'"

<latent_space> ::= encoder.add(Dense(units = <LatentUnits>, activation = <activation>, name="'latent'"))

<LatentUnits> ::= 4 | 8 | 16 | 32

<digit> ::= 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
